# src/preprocess.py

"""
Data cleaning, normalization, and feature engineering.
This script reads the raw data, cleans it, adds new features,
and saves the result. It also triggers the visualization script.

Run from project root:
    python src/preprocess.py
"""

import os
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from typing import List, Any

# Import the main function from the visualization script
try:
    from visualize import main as run_visualizations
except ImportError:
    run_visualizations = None

# --- Constants ---
# Assumes this script is in 'src/'
BASE_DIR = os.path.dirname(__file__)
# Input data (the one generated by utils.py)
INPUT_PATH = os.path.join(BASE_DIR, "..", "data", "processed", "tlr_model_input.csv")
# Output path for the newly processed data
OUTPUT_PATH = os.path.join(BASE_DIR, "..", "data", "processed", "tlr_model_features.csv")


def load_data(path: str) -> pd.DataFrame:
    """
    Loads data from a specified CSV path.
    """
    if not os.path.exists(path):
        raise FileNotFoundError(
            f"Data file not found at {path}\n"
            "Please run 'python src/utils.py' first to generate the data."
        )
    print(f"Loading data from {path}...")
    return pd.read_csv(path)


def clean_dataframe(df: pd.DataFrame) -> pd.DataFrame:
    """
    Cleans the dataframe:
    - Fills missing numeric values with the column median.
    - Fills missing object/string values with 'Unknown'.
    """
    print("Cleaning dataframe (handling missing values)...")
    
    # Handle numeric NaNs
    # This is robust for future "real" data
    numeric_cols = df.select_dtypes(include=np.number).columns
    for col in numeric_cols:
        median = df[col].median()
        df[col] = df[col].fillna(median)
    
    # Handle object/categorical NaNs
    object_cols = df.select_dtypes(include='object').columns
    for col in object_cols:
        df[col] = df[col].fillna('Unknown')
        
    return df


def add_features(df: pd.DataFrame) -> pd.DataFrame:
    """
    Engineers new features from existing data.
    These features can help the model find better relationships.
    """
    print("Engineering new features...")
    
    # 1. PhD per Faculty Ratio (proxy for FQE)
    # Use np.where to avoid division by zero
    df['phd_per_faculty_ratio'] = np.where(
        df['faculty_count'] > 0,
        df['phd_faculty_count'] / df['faculty_count'],
        0
    )
    
    # 2. Students per Faculty Ratio (proxy for FSR)
    df['students_per_faculty'] = np.where(
        df['faculty_count'] > 0,
        df['student_count'] / df['faculty_count'],
        0
    )
    
    # 3. Total Expense per Student (proxy for FRU)
    df['total_expense_per_student'] = np.where(
        df['student_count'] > 0,
        (df['library_expense'] + df['lab_expense']) / df['student_count'],
        0
    )
    
    return df


def normalize_data(df: pd.DataFrame, columns: List[str]) -> pd.DataFrame:
    """
    Applies Min-Max scaling (0-to-1) to specified columns.
    """
    print(f"Normalizing {len(columns)} feature columns...")
    scaler = MinMaxScaler()
    
    # Fit and transform the specified columns
    df[columns] = scaler.fit_transform(df[columns])
    
    return df


def main():
    """
    Runs the full preprocessing pipeline:
    Load -> Clean -> Engineer Features -> Normalize -> Save -> Visualize
    """
    print("--- Starting Preprocessing Pipeline ---")
    
    # 1. Load Data
    try:
        raw_df = load_data(INPUT_PATH)
    except FileNotFoundError as e:
        print(e)
        return

    # 2. Clean Data
    # .copy() is used to avoid pandas SettingWithCopyWarning
    clean_df = clean_dataframe(raw_df.copy())
    
    # 3. Engineer Features
    featured_df = add_features(clean_df)
    
    # 4. Normalize Data
    # Define which columns to scale.
    # We scale the original scores + our new features
    # We exclude identifiers, ranks, and raw counts
    columns_to_scale = [
        'ss_score', 'fsr_score', 'fqe_score', 'fru_score', 'oe_score', 'mir_score',
        'phd_per_faculty_ratio', 'students_per_faculty', 'total_expense_per_student'
    ]
    
    # Ensure all columns exist before trying to scale
    valid_columns_to_scale = [col for col in columns_to_scale if col in featured_df.columns]
    
    normalized_df = normalize_data(featured_df, valid_columns_to_scale)
    
    # 5. Save Data
    # Ensure the output directory exists
    os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)
    normalized_df.to_csv(OUTPUT_PATH, index=False)
    
    print(f"\nâœ… Preprocessing complete!")
    print(f"Processed data saved to: {OUTPUT_PATH}")
    print("\n--- Processed Data Head ---")
    print(normalized_df.head())

    # --- 6. Run Visualizations ---
    if run_visualizations:
        print("\n--- Triggering Visualization Script ---")
        try:
            run_visualizations()
        except Exception as e:
            print(f"Could not run visualization script: {e}")
    else:
        print("\nCould not import visualize.py, skipping visualization step.")


if __name__ == "__main__":
    main()